The only change here is the MDP
In this toy MDP problem that we have considered is just to check the validity of the algorithm
transition probability[0] = [[0,1],
                             [1,0]].
transition_probability[1] = [[1, 0],
                             [0, 1]].
behaviour_policy = np.array([[0.5,0.5],[1,0]])
policies = np.array([[0,0]])


Will update this later
